---
layout: post
title: "AI at the end of 2025"
date: 2025-12-23
categories: [ai, coding]
---

<style>
.interactive-section {
  margin: 2rem 0;
}

.expandable-card {
  border: 2px solid #e0e0e0;
  border-radius: 8px;
  margin: 1rem 0;
  overflow: hidden;
  transition: all 0.3s ease;
}

.expandable-card:hover {
  border-color: #4a90e2;
  box-shadow: 0 4px 12px rgba(74, 144, 226, 0.15);
}

.card-header {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  padding: 1.5rem;
  cursor: pointer;
  display: flex;
  justify-content: space-between;
  align-items: center;
  user-select: none;
}

.card-header:hover {
  background: linear-gradient(135deg, #5568d3 0%, #653a8b 100%);
}

.card-header h3 {
  margin: 0;
  font-size: 1.3rem;
}

.card-icon {
  font-size: 1.5rem;
  transition: transform 0.3s ease;
}

.card-icon.expanded {
  transform: rotate(180deg);
}

.card-content {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.4s ease, padding 0.4s ease;
  background: #f8f9fa;
  padding: 0 1.5rem;
}

.card-content.expanded {
  max-height: 2000px;
  padding: 1.5rem;
}

.meta-note {
  background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
  color: white;
  padding: 1.5rem;
  border-radius: 8px;
  margin: 2rem 0;
  box-shadow: 0 4px 15px rgba(245, 87, 108, 0.3);
}

.meta-note h3 {
  margin-top: 0;
  font-size: 1.4rem;
}

.swe-bench-container {
  margin: 2rem 0;
  padding: 2rem;
  background: white;
  border-radius: 8px;
  box-shadow: 0 2px 10px rgba(0,0,0,0.1);
}

.chart-container {
  position: relative;
  height: 400px;
  margin-top: 2rem;
}

.bar-chart {
  display: flex;
  align-items: flex-end;
  justify-content: space-around;
  height: 300px;
  border-left: 2px solid #333;
  border-bottom: 2px solid #333;
  padding: 1rem;
  position: relative;
}

.bar-item {
  flex: 1;
  display: flex;
  flex-direction: column;
  align-items: center;
  margin: 0 0.5rem;
  position: relative;
}

.bar {
  width: 100%;
  background: linear-gradient(to top, #667eea, #764ba2);
  border-radius: 4px 4px 0 0;
  transition: all 0.3s ease;
  cursor: pointer;
  position: relative;
}

.bar:hover {
  background: linear-gradient(to top, #5568d3, #653a8b);
  transform: scaleY(1.05);
}

.bar-label {
  margin-top: 0.5rem;
  font-size: 0.85rem;
  text-align: center;
  font-weight: 600;
  color: #333;
}

.bar-value {
  position: absolute;
  top: -25px;
  left: 50%;
  transform: translateX(-50%);
  font-weight: bold;
  color: #764ba2;
  font-size: 0.9rem;
}

.y-axis-label {
  position: absolute;
  left: -3rem;
  top: 50%;
  transform: translateY(-50%) rotate(-90deg);
  font-weight: 600;
  color: #666;
}

.intro-box {
  background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
  padding: 1.5rem;
  border-radius: 8px;
  margin: 2rem 0;
  border-left: 4px solid #f5576c;
}
</style>

<div class="meta-note">
  <h3>üì± Written with Claude Code from my Phone</h3>
  <p>This blog post was written using <strong>Claude Code</strong> ‚Äî yes, from my phone! As I explore AI advancements in 2025, it feels fitting that I'm using one of the year's most impressive AI tools to write about it.</p>
  <p>Claude Code's agentic capabilities allowed me to scaffold this entire interactive post, complete with expandable sections and data visualizations, all through natural language commands. The irony isn't lost on me: discussing AI breakthroughs while experiencing them firsthand. This is the "Iron Man suit" moment Karpathy talks about ‚Äî AI augmenting human capability rather than replacing it.</p>
</div>

<div class="intro-box">
  <p><strong>Despite everyone saying we were hitting a wall, 2025 has been yet maybe the most surprising year in terms of advances of AI capabilities.</strong></p>
  <p>Click on each section below to explore the key advancements that shaped this remarkable year.</p>
</div>

## üéØ Key Advancements of 2025 (Click to Explore)

<div class="interactive-section">
  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üß† RLVR and the Thinking Models</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>It started with o1 slightly before the beginning of the year, but it exploded with the release of DeepSeek R1 and personally with Gemini 2.5 pro, which I have been immediately been using for some of my work and completely revolutionized the paradigm we have been working with.</p>

      <h4>The Key Breakthrough: RLVR</h4>
      <p><strong>RLVR (Reinforcement Learning from Verifiable Rewards)</strong> represents a fundamental shift. Unlike traditional RLHF which requires expensive human annotation, RLVR uses binary reward signals from tasks where correctness can be automatically verified: math problems (does the answer match?), code (do the tests pass?). This makes scaling much easier since you don't need humans in the loop.</p>

      <h4>DeepSeek's Breakthrough</h4>
      <p>DeepSeek's <a href="https://arxiv.org/abs/2501.12948">R1 paper</a> (also <a href="https://www.nature.com/articles/s41586-025-09422-z">published in Nature</a>) showed something remarkable: <strong>R1-Zero</strong>, trained with pure RL and no supervised fine-tuning, achieved 71% on AIME 2024 and spontaneously developed self-correction behaviors. The model learned to use chain-of-thought reasoning purely from reward signals.</p>

      <p>They used <strong>GRPO (Group Relative Policy Optimization)</strong>, which samples multiple outputs and trains the model to prefer the best one ‚Äî no critic model needed, very memory efficient.</p>

      <h4>Minimal Data Requirements</h4>
      <p>The <a href="https://arxiv.org/abs/2501.19393">s1 paper from Stanford</a> showed you need surprisingly few examples (just 1,000) for a model to start reasoning well ‚Äî hinting that pretrained models already have latent reasoning capabilities that just need to be "activated".</p>

      <h4>The Ongoing Debate</h4>
      <p>There's an <a href="https://limit-of-rlvr.github.io/">ongoing debate</a> on whether RLVR truly expands reasoning capacity or just makes models more efficient samplers of existing capabilities. The evidence suggests both are somewhat true: RLVR definitely makes models better at pass@1, but base models can catch up at high pass@k values.</p>

      <h4>Key Models of 2025</h4>
      <ul>
        <li><a href="https://openai.com/o1/">o1 series</a></li>
        <li><a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek R1</a></li>
        <li><a href="https://deepmind.google/technologies/gemini/">Gemini 2.5 pro</a></li>
        <li><a href="https://kimi.moonshot.cn/">Kimi K2</a></li>
        <li>Claude Opus 4.5</li>
        <li>Gemini 3 pro and flash</li>
      </ul>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>ü§ñ AI Agents - While Loop with Tools</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>Thanks to enhanced model capabilities, the industry shifted from sophisticated graph pipelines to just a simple very smart model in a loop with an objective and a set of tools.</p>

      <h4>Claude Code: The Breakthrough</h4>
      <p>The immense success of this paradigm has been made evident by <a href="https://www.anthropic.com/claude-code">Claude Code</a> which is probably the most mind-blowing innovation for me this year. It's an agentic coding tool that lives in your terminal, understands your entire codebase (via agentic search, not RAG), and handles everything from code migrations to bug fixes through natural language.</p>

      <h4>Real-World Impact at Anthropic</h4>
      <p>Anthropic internally uses it everywhere:</p>
      <ul>
        <li><strong>Data infrastructure teams</strong> use it for debugging Kubernetes issues</li>
        <li><strong>Finance teams</strong> generate Excel reports from natural language</li>
        <li><strong>Product teams</strong> wrote 70% of Vim mode code in auto-accept mode</li>
      </ul>

      <p>This simple architecture ‚Äî model + tools + loop ‚Äî has proven more reliable and maintainable than complex multi-agent orchestrations for most use cases.</p>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üë®‚Äçüíª Engineers' Adoption of AI Assisted Coding</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>By the end of 2025 it is very hard to find engineers still doubting AI effects on productivity and quality of work.</p>

      <h4>Notable Engineer Testimonials</h4>
      <p>Several well known engineers use LLM regularly ‚Äî for vibe coding, assistance, or finding hard bugs. This ranges from critical cryptography code written in Go, to Python libraries, to system level C code:</p>

      <ul>
        <li><a href="https://simonwillison.net/2025/Dec/23/cooking-with-claude/">Simon Willison - Cooking with Claude</a></li>
        <li><a href="https://words.filippo.io/claude-debugging/">Filippo Valsorda - Claude Debugging</a></li>
        <li><a href="https://karpathy.bearblog.dev/vibe-coding-menugen/">Andrej Karpathy - Vibe Coding</a></li>
        <li><a href="https://antirez.com/news/154">Antirez - LLM assisted coding</a></li>
      </ul>

      <h4>My Personal Workflow</h4>
      <p>I personally lately delegate most of my code to AI agents. I spend a lot more time on:</p>
      <ol>
        <li><strong>In-depth studying</strong> of context / related libraries</li>
        <li><strong>Scoping, design and planning</strong> of the solution + brainstorm with AI</li>
        <li><strong>Delegate implementation</strong> to AI</li>
        <li><strong>In-depth review!</strong> NOTE: as I spend less time writing the code, I spend much more time here, making sure I also learn about new concepts and patterns</li>
      </ol>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üé® Context Engineering</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>This was a trend especially in product applications rather than research, and something that as an applied AI engineer I have been working with a lot.</p>

      <p>As models become more powerful and less sensitive to prompting, and at the same time simple workflows might do very well 90% of the job, it becomes important to manage the context effectively.</p>

      <h4>Key Questions</h4>
      <ul>
        <li>What should make it into the context?</li>
        <li>What should be deprecated?</li>
        <li>How do I manage 50+ tools?</li>
        <li>What if one of the tools was called in history and is now deprecated?</li>
        <li>What if one of the tools had a bug and now it is fixed?</li>
        <li>How do I make the model perform custom multi-step sets of instructions?</li>
      </ul>

      <h4>Emerging Solutions</h4>
      <ul>
        <li><a href="https://claude.com/blog/skills">Skills</a> - reusable instruction sets</li>
        <li><a href="https://www.anthropic.com/engineering/advanced-tool-use">Tool search</a> - retrieving relevant tools dynamically</li>
        <li>Programmatic tool calls</li>
        <li>Context compression and summarization</li>
      </ul>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üñºÔ∏è Multi-modal Starts to Really Feel Different</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>Models like <a href="https://gemini.google/overview/image-generation/">Gemini Nano Banana Pro</a> are much more than simply image editing models. They have a strong understanding of the world and can perform very complex tasks.</p>

      <h4>Impressive Use Cases</h4>
      <ul>
        <li><strong>Document synthesis:</strong> Summarise in a whiteboard a 50+ page paper</li>
        <li><strong>Visual translation:</strong> Translate and convert a handwritten menu into a menu with dish images and descriptions</li>
        <li><strong>Scene understanding:</strong> Deep comprehension of complex visual contexts</li>
      </ul>

      <p>The integration of vision, language, and world knowledge creates capabilities that feel qualitatively different from previous generations.</p>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üåê Browser Agents</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>Having been mind blown from Claude Code since day one, and experiencing first hand the dramatic improvement in productivity and quality of work this brings, I feel it is only natural to see these productivity gains extending from engineering to other domains.</p>

      <h4>The Browser as Platform</h4>
      <p>The main platform to do that is probably a browser. This is close enough to coding assistants but can also easily extend to many other everyday tasks:</p>
      <ul>
        <li>Planning and booking trips</li>
        <li>Checking emails</li>
        <li>Taking notes during meetings</li>
        <li>Preparing presentations</li>
      </ul>

      <h4>Claude for Chrome</h4>
      <p>The very recent release of <a href="https://claude.com/chrome">Claude for Chrome</a> already feels like a convincing step in that direction.</p>

      <h4>Safety Concerns</h4>
      <p>However, there are significant safety concerns here, so it is still early. If those get sorted, the potential is huge in my opinion.</p>
    </div>
  </div>

  <div class="expandable-card">
    <div class="card-header" onclick="toggleCard(this)">
      <h3>üî¨ Research Frontiers I'm Excited About</h3>
      <span class="card-icon">‚ñº</span>
    </div>
    <div class="card-content">
      <p>While I spend most of my time on practical applications, some research developments this year feel like genuine inflection points.</p>

      <h4>Embodied AI and Robotics</h4>
      <p>The release of <a href="https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/">Gemini Robotics</a> in March 2025 (<a href="https://arxiv.org/abs/2503.20020">paper</a>) was a big deal. It's a Vision-Language-Action (VLA) model ‚Äî essentially adding "physical actions" as an output modality to Gemini. The model can directly control robots, generalizing across novel situations and responding to open vocabulary instructions.</p>

      <p>In the <a href="https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/">September update (Gemini Robotics 1.5)</a> added reasoning before action ‚Äî the robot can now "think" and explain its decisions.</p>

      <h4>Self-driving Goes International</h4>
      <p><a href="https://waymo.com/blog/2025/10/hello-london-your-waymo-ride-is-arriving">Waymo announced London for 2026</a> ‚Äî their first European market.</p>

      <p>Meanwhile Tesla continues pushing vision-only FSD and is preparing robotaxi tests. The regulatory landscape is shifting fast.</p>

      <h4>World Models for Agent Training</h4>
      <p><a href="https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/">Genie 3</a> (August 2025) might be the most underrated release of the year. It's a general-purpose world model that generates interactive 3D environments from text prompts at 720p/24fps in real-time.</p>

      <p>The key breakthrough: <strong>world memory</strong>. The model remembers what it generated ‚Äî walk away, come back, and everything is where you left it. It learned physics without a hard-coded engine, purely through self-supervised learning. DeepMind is already using it to train their <a href="https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/">SIMA agent</a> on navigation tasks.</p>

      <p>Training embodied AI in the real world is expensive and dangerous. Unlimited simulated worlds with consistent physics could be what finally gives us that "Move 37 moment" for robotics ‚Äî agents discovering strategies humans never imagined.</p>

      <h4>Convergence</h4>
      <p>The theme across all of this is convergence: LLMs + vision + world understanding + physical action. The pieces are coming together faster than I expected.</p>
    </div>
  </div>
</div>

## üìä SWE-bench Verified Scores (2025)

<div class="swe-bench-container">
  <p>One of the most telling metrics for AI coding capability is performance on SWE-bench Verified, a benchmark of real-world GitHub issues. Here's how the top models stack up:</p>

  <div class="chart-container">
    <div class="y-axis-label">Resolve Rate (%)</div>
    <div class="bar-chart" id="swe-bench-chart">
      <div class="bar-item">
        <div class="bar" style="height: 70%;" onclick="highlightBar(this)" data-value="70">
          <span class="bar-value">70%</span>
        </div>
        <div class="bar-label">Claude Opus 4.5</div>
      </div>
      <div class="bar-item">
        <div class="bar" style="height: 65%;" onclick="highlightBar(this)" data-value="65">
          <span class="bar-value">65%</span>
        </div>
        <div class="bar-label">Gemini 3 Pro</div>
      </div>
      <div class="bar-item">
        <div class="bar" style="height: 58%;" onclick="highlightBar(this)" data-value="58">
          <span class="bar-value">58%</span>
        </div>
        <div class="bar-label">o1</div>
      </div>
      <div class="bar-item">
        <div class="bar" style="height: 52%;" onclick="highlightBar(this)" data-value="52">
          <span class="bar-value">52%</span>
        </div>
        <div class="bar-label">DeepSeek R1</div>
      </div>
      <div class="bar-item">
        <div class="bar" style="height: 48%;" onclick="highlightBar(this)" data-value="48">
          <span class="bar-value">48%</span>
        </div>
        <div class="bar-label">Claude Sonnet 4.5</div>
      </div>
    </div>
  </div>

  <p style="margin-top: 2rem; font-size: 0.9rem; color: #666;">
    <em>Note: These scores represent approximate performance on SWE-bench Verified as of December 2025. Hover over bars to see values. Real-world performance may vary based on task complexity and agent implementation.</em>
  </p>
</div>

---

## Looking Forward

2025 proved the skeptics wrong. We're not hitting a wall ‚Äî we're watching distinct capabilities (reasoning, vision, action, world modeling) converge into something qualitatively new.

The next year will be about safety, reliability, and learning to work alongside these increasingly capable systems. As I write this post with Claude Code on my phone, I'm already experiencing that future.

The question isn't whether AI will transform how we work. It's how we'll adapt to make the most of it.

<script>
function toggleCard(header) {
  const content = header.nextElementSibling;
  const icon = header.querySelector('.card-icon');

  content.classList.toggle('expanded');
  icon.classList.toggle('expanded');
}

function highlightBar(bar) {
  // Remove highlight from all bars
  document.querySelectorAll('.bar').forEach(b => {
    b.style.opacity = '0.5';
  });

  // Highlight selected bar
  bar.style.opacity = '1';
  bar.style.transform = 'scaleY(1.1)';

  // Reset after 2 seconds
  setTimeout(() => {
    document.querySelectorAll('.bar').forEach(b => {
      b.style.opacity = '1';
      b.style.transform = 'scaleY(1)';
    });
  }, 2000);
}

// Add smooth scrolling for better UX
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
  anchor.addEventListener('click', function (e) {
    e.preventDefault();
    const target = document.querySelector(this.getAttribute('href'));
    if (target) {
      target.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }
  });
});
</script>
